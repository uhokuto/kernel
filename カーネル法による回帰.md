# カーネル法を用いた回帰

カーネル法は、再生核ヒルベルト空間上でデザインした関数$f$によって本来は非線形な回帰問題を線形にするものである。したがって,

- カーネル法のご利益である線形問題への変換がいかに実現できるのか

- カーネル法による回帰問題において関数$f$はどのような条件（デザイン）をもつのか
  
  を理解することが非常に重要である。以下はその解説である。要約すると、

**再生核ヒルベルト空間上のカーネルなので、カーネルに関する内積$\langle f,k_x \rangle$のような条件を満たす。 $f$は1変数カーネルの線形和$\sum_jc_jk_{x_j}(x)$にデザインできる。この式は、$x_j$をN個の観測データとすると、N×Nのカーネル行列と$c_j$との内積そのものである。またカーネル$k$に特定の縛りはない（デザインフリー）**

### 1. $f$は再生核ヒルベルト空間上の関数であることを条件とした途端に、カーネルの線形和で表せる

再生核ヒルベルト空間上の$f=\langle f,k_x \rangle_H$において、$\{k_1,k_2,...k_n\}$は$H$上の部分空間をなす。ということは、この再生核等式の意味することは、$f$からこの部分空間への射影(=内積)の値が、$f$を等しいような空間であることを意味している。

ここで$k_x$をカーネルとするならば、任意の$f \in  H$は$k_n$の線形和で表せる。すなわち、

$f=\sum_{j=1}^nc_jk_{x_k}$

と表すことができる

以下その証明

$P$を$f$から$\{k_1,k_2,...k_n\}$への射影行列とすると、$f$は直交分解できて、$f=Pf + (f - Pf)$が成り立つ。ここで$f-Pf$は$\{k_1,k_2,...k_n\}$の各要素と直交する。このことから以下が成り立つ

    $f(x_i)=\langle f,k_{x_i} \rangle_H = \langle Pf + (f - Pf),k_{x_i} \rangle_H$

    $=\langle Pf ,k_{x_i} \rangle_H$ 

   $=(Pf)(x_i)$  : $Pf$は部分空間上$\{k_1,k_2,...k_n\}$にあるから上式は、$Pf$の線形結合

以上により、$f=Pf=\sum_{j=1}^nc_jk_{x_k}$

### 2. 再生核ヒルベルト空間上の任意の$f$は、カーネル行列で表すことができる。この際、カーネルの形には制約はない

1.において、$f$がカーネル線形和で表せることがわかったので、カーネルによるヒルベルト空間の性質を用いて以下のように$f$をカーネル行列に変換できる

$f(x_i)=\langle f,k_{x_i} \rangle_H =\langle \sum_{j=1}^nc_jk_{x_j},k_{x_i} \rangle_H $

$=\sum_{j=1}^nc_jk(x_i,x_j)$ : 　(1)

(最後の展開は、「カーネル関数から再生核ヒルベルト空間を構成する方法」を参照のこと)

この式において、**カーネル$k$には何ら制約条件がないことに注意**

この式を行列形式に表すと、行列の$i,j$要素がカーネル$k(x_j,x_j)$であるようなカーネル行列にベクトル$c_j$をかけた式となる。これは、**$f$をカーネル$k$の$c$による線形結合式に変換したことを意味する**これが得られれば、非線形な関数$f$お、以下のような最小二乗回帰で解くことができる

### 3. カーネル法による線形回帰

(1)式について、$k(x_i,x_j)=K$, $\{c_1,..c_n\}=C$とすると、目的変数$\lambda$，説明変数$x$についての非線形関数$f$は、以下のような線形回帰評価関数に変換することができる。

評価関数$L(f)=\sum_{j=1}^n|f(x_j)-\lambda_j|^2=||KC-\lambda||_{R^n}^2$ 

上式を最小化する$C$を最小二乗法で解くと、関数$f$をカーネルの線形結合で表せる

$f=\sum_{j=1}^nc_jk_{x_j}$



### 4. カーネル法による線形回帰においてカーネル$k$をデザインする方法

**$\nu$中の$k$を選択する**

回帰モデル$f$ :$\lambda_j = f(x_j)$をカーネル法による線形回帰に変換するには、$f\in\nu$になるような$f$をデザインする必要がある。ここで、

$\nu=\{ \sum_j^d a_jk_{x_j}\}$

- カーネルは、$k_{x_j}=k(x,x_j)$のように$x_j$を固定値とした1変数関数

- $a_jk_{x_j}$は互いに線形独立

である。すなわち$\nu$は、カーネル$k_{x_j}$を$d$個のベクトルとする$d$次元カーネルベクトル空間上の関数$f$の集合である。



### 5. 多項式カーネルによる線形回帰

1. 多項式カーネルをデザインする

        カーネルの条件を満たすように多項式をデザインすると

            $P_d=k_y(x)=1 + yx + .... + y^dx^d$　(1)

 -  これは、$y$の多項式であって$x$の多項式ではないことに注意

 - $y$は、$d+1$個の観測データである



    2.  多項式カーネル関数(1)からの再生核ヒルベルト空間の構成

        (1)式を$d+1$個ならべたカーネル集合$\{k_{y_1}(x),...,k_{y_d+1}(x)\}$ は線形独立である。

        なので、(1)の線形結合の集合（下式）は$\nu$上にあるといえる。

          $\bigl\{\sum_{j}{a_jk_{y_j}(x)}\bigr\}= \{a_0+a_1x+a_2x^2...+a_dx^d\}=\{p(x)\} \in \nu$

            -  $\nu$は$k_{x_j}$を基底とするカーネル関数のベクトル空間を意味する。

            - $k_{y_j}=k_{y_j}(x)$で$x$は変数だから、ベクトル空間でなくて、関数空間

        カーネル$k$が$p\in\nu$を満たすなら、$k$は再生核ヒルベルト空間を構成でき、再生核等式         $p(y)=\langle\bm{p},\bm{k_y}\rangle_{\nu}$が成り立つ。

        $p(y)$は$p(x)$の$x$に$y$の代入を意味する。最後の式では$y_j$を省略している　p.66 例4.1.6



3. 多項式カーネル関数による線形回帰モデルの構築

    2.の再生核等式を再掲する。

    $p(y)=\langle\bm{p},\bm{k_y}\rangle_{\nu}=a_0+a_1y+..+a_dy^d$     (2)

    これは、$x$に固定値$y$を代入すると、$p(x)=\sum_ja_jk_{y_j}(x)$と$k_y(x)$の内積から$x$が消えることを意味する。上式で$y_i$を$n$個の観測データ、$x$も$y_j$とすると、

        $p(y_i)=\langle\bm{p},\bm{k_{y_i}}\rangle_{\nu}=\langle\bm{\sum_{j=1}}^na_jy_j,\bm{k_{y_i}}\rangle_{\nu}=\sum_{j=1}^na_jk(y_i,y_j)$

    最後の式展開は、p.65上の展開による。この最後の式の$k$は、(1)式で$x$を$y_i$と置いた式に他ならない。このことから、問題$L(p)=\sum_{j=1}^{n}{|p(x_j)-\lambda|^2}$は、$x_j$を$y_i$に読み替えた多項式カーネル関数(1)=$k_{y_i}(y_j)=k(y_i,y_j)$を以下のような線形回帰問題に還元できることがわかる。

        $L(p)=\sum_{i=1}^{n}{|p(y_i)-\lambda|^2}=\sum_{i=1}^{n}{|\sum_{j=1}^na_jk(y_i,y_j)-\lambda|^2} $

- 推定したい未知パラメータは$a_j$

- (2)式から$a_j$を推定できそうだが、カーネル行列にしないと推定できない？

- 多項式カーネル関数の次元数$d$は観測データ数でgiven

- この式の内側の$\sum$は$i:1～n$に対して、p.72 (4.2.1)のカーネル行列そのものである。（$f(x_n) -> p(y_i)$と置き換えたものである）

## **カーネル線形回帰におけるカーネルの構築方法**

上例の多項式カーネル関数のとおり、以下のような関数を設計すれば、カーネルの線形回帰問題に帰着できる

- 関数がカーネルであること

- そのカーネルは、集合 $\nu$上の関数空間であること

リプレゼンター定理は、回帰問題にカーネルを適用する際の一般的なカーネル設計方法を言っている
